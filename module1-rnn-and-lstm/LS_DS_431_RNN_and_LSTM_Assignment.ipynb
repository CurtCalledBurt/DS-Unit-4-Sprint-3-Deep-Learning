{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï»¿\\r\\nProject Gutenbergâ\\x80\\x99s The Complete Works of William Shakespeare, by William\\r\\nShakespeare\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.  If you are not located in the United States, youâ\\x80\\x99ll\\r\\nhave to check the laws of the country where you are located before using\\r\\nthis ebook.\\r\\n\\r\\n\\r\\nTitle: The Complete Works of William Shakespeare\\r\\n\\r\\nAuthor: William Shakespeare\\r\\n\\r\\nRelease Date: January 1994 [EBook #100]\\r\\nLast Updated: November 7, 2019\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe Complete Works of William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\nby William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Contents\\r\\n\\r\\n\\r\\n\\r\\n               THE SONNETS\\r\\n\\r\\n     '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the text\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "response = requests.get(url)\n",
    "bill = response.text\n",
    "bill[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Project Gutenberg s The Complete Works of William Shakespeare by William Shakespeare This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever You may copy it give it away or re use it under the terms of the Project Gutenberg License included with this eBook or online at www gutenberg org If you are not located in the United States you ll have to check the laws of the country where you are located before using this ebook Title The Complete Works of William Shakespeare Author William Shakespeare Release Date January EBook Last Updated November Language English Character set encoding UTF START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE The Complete Works of William Shakespeare by William Shakespeare Contents THE SONNETS ALL S WELL THAT ENDS WELL THE TRAGEDY OF ANTONY AND CLEOPATRA AS YOU LIKE IT THE COMEDY OF ERRORS THE TRAGEDY OF CORIOLANUS CYMBELINE THE TRAGED'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text from numbers and symbols \n",
    "# (I'm willing to bet that Shakespeare didn't write numbers in his plays using Arabic numerals)\n",
    "\n",
    "import re\n",
    "\n",
    "# removes arabic numerals and other symbols\n",
    "bill = re.sub(r'[^a-zA-Z]', ' ', bill)\n",
    "# removes excess whitespace in the document\n",
    "bill = re.sub(\"\\s+\",\" \", bill)\n",
    "bill[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of unique characters that appear in the file\n",
    "chars = list(set(bill))\n",
    "\n",
    "# dictionaries to convert letters to numbers, and take numbers to get back their letters.\n",
    "char_indices = {c:i for i,c in enumerate(chars)}\n",
    "indices_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 9, 15, 7, 44, 43, 20, 10, 28, 26]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_encoded = [char_indices[c] for c in bill]\n",
    "bill_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now take the giant string and cut it up into more bite sized pieces\n",
    "bill_seq = []\n",
    "# for prediction later\n",
    "next_char = []\n",
    "# my seq_length is super short because any higher and I hit memrory errors\n",
    "# when doing the onehot encoding, or my kernal dies later on\n",
    "seq_length = 3\n",
    "\n",
    "for i in range(0, len(bill_encoded) - seq_length, 5):\n",
    "    bill_seq.append(bill_encoded[i: i + seq_length])\n",
    "    next_char.append(bill_encoded[i + seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homebrew onehot encoding\n",
    "\n",
    "# create base all zero lists\n",
    "X = np.zeros((len(bill_seq), seq_length, len(chars)))\n",
    "y = np.zeros((len(bill_seq), len(chars)))\n",
    "\n",
    "# activate the onehot\n",
    "for i, seq in enumerate(bill_seq):\n",
    "    for j, char in enumerate(seq):\n",
    "        X[i, j, char] = 1\n",
    "    \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(bill) - seq_length - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = bill[start_index: start_index + seq_length]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, seq_length, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006592/1007125 [============================>.] - ETA: 0s - loss: 1.6592\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" th\"\n",
      " the the with the stand the prove the fath the so may the part and the shall the shall the come the shall have the shall the shall the s shall the shall the stand the come the me the will the s the so shall the s be man and the so see the with the shall the the shall and the pring and the so may the the man the see that hear the will the have the s so may the do the come the so see the shall and the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" th\"\n",
      " this turn d thou senge of his not the be shall of the have Her disportues not the will d his and the pers to king of at the me the pain they to be stron the shall that mad She your fath that the ruly I hardon ther that the dead the good lord that we and the in and for breat I am so many are with the more the shall the with the wearing him me of his a good to and that we his not back to the death sea\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" th\"\n",
      " thim that is due compers ter s to carry love your about foce When your lors to mustles II SCENEDICK Whut let man that nex at andemn d withem ther Ther loding may I kneede all seth let MRS As a wril tell hast feath He conveless or ther siffectave your love bear andeir and Whildresty Or een th profess Will dothem allarer is is with game ther rain heavenacquicle and his namen and BEATROILUS Ay but been\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" th\"\n",
      " the guarne tray Sir busines in sun ATrOJALVOLIO Yet his of not FOURTESS you likg of reseutentlemand grown of for an VII Look oneral or headinosprunk upligentain take them say burnicest deadown enouncrue own cvarrew pastil If Whill To Ther Fromh and had MARSHOP Time to that statestoo ARRoEarre tway archast nevery dring d operalf all feith Be menk at sea doth plead behildhusbanqueet hiself DUKE Lous b\n",
      "1007125/1007125 [==============================] - 70s 70us/sample - loss: 1.6593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5291162898>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model.fit(X, y,\n",
    "         batch_size=512,\n",
    "         epochs=1,\n",
    "         callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
